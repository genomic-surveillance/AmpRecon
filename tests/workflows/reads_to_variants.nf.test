nextflow_workflow {

    name "Test Workflow READS_TO_VARIANTS"
    script "workflows/reads_to_variants.nf"
    workflow "READS_TO_VARIANTS"

    def test_dir = "/nfs/team335/production/resources/pipeline_resources/plasmodium/falciparum/amplicon/datasets/workflow-testing/"
    def cont_dir = "/lustre/scratch126/gsu/malariagen/production/resources/pipeline_resources/plasmodium/falciparum/amplicon/ampseq_containers/"
    
    test("Should run without failures and output VCFs should be correct") {

        when {
            params {
                // define parameters here. Example:
                // outdir = "tests/results"
                genotyping_gatk = false
                genotyping_bcftools = true
                channel_data_file = "${test_dir}/reads_to_variants/reads_to_variants_workflow_data.tsv"
                containers_dir = "${cont_dir}"
            }

            workflow {
                """
                // define inputs of the workflow
                channel_data = Channel.fromPath(params.channel_data_file, checkIfExists: true).splitCsv(header: true, sep: '\t')

                // set Reads to Variants input channels
                fastq_ch = channel_data.map { row -> tuple(row.file_id, row.fastq_file, row.reference_file) }
                file_id_reference_files_ch = channel_data.map { row -> tuple(row.file_id, row.panel_name, row.reference_file, row.snp_list) }
                annotations_ch = channel_data.map { row -> tuple(row.panel_name, file(row.annotation_file)) }.unique()
                file_id_to_sample_id_ch = channel_data.map { row -> tuple(row.file_id, row.sample_id) }

                input[0] = fastq_ch
                input[1] = file_id_reference_files_ch
                input[2] = annotations_ch
                input[3] = file_id_to_sample_id_ch
                """
            }
        }

        then {
            // Assert workflow succeeded
            assert workflow.success

            // MD5 hashes to check against
            def ref_md5_hashes = ["12345_PFA_Spec.bcftools_genotyped.vcf.gz": "448372d3fd3acc34db45d0d465dc779c", 
            "12345_PFA_GRC1_v1.0.bcftools_genotyped.vcf.gz": "5078c8d89a3f0b7dcdeadcff5c8dd108", 
            "12345_PFA_GRC2_v1.0.bcftools_genotyped.vcf.gz": "338e6d3b6ac8701cdb9cfa9b7bd0ca40"]

            // Retrieve ID and VCF path from each line in the lanelet VCF manifest
            with(workflow.out.lanelet_manifest){
                new File(get(0)).eachLine { line ->
                    def (id, vcf) =  line.split(",")
                    if (vcf != "vcf_path") {

                        // Drop dates and paths from the header of each VCF
                        // Get the resulting MD5 hash
                        def command = "zcat ${vcf} | awk '{gsub(/Date=.*/,DateRemoved)}1' | sed -E '/^##/ s|/\\S*/([^/]*)|\\1|g' | md5sum | awk '{ printf \$1 }'"
                        def data_lines_md5_hash = ['bash', '-c', command].execute().in.text

                        // Compare MD5 hash of generated MD5 hashes to reference MD5 hashes
                        String vcf_name = file(vcf).getName()
                        assert ref_md5_hashes.get(vcf_name) == data_lines_md5_hash
                    }
                }
            }
        }
    }
}
